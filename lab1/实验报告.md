# Web lab1 实验报告
> PB18000037 肖桐 PB18071521 高路尧

## 实验说明
我们进行实验对数据进行了预处理操作方便存储以及数据的读写.

具体在于将5个文件夹的内容进行综合到了5个json文件中, 并命名为`2018_01.json 2018_02.json 2018_03.json 2018_04.json 2018_05.json` 

对每个文章选择顺序的方式赋予新的ID, 方便进行数据的存储和选择, 记录为`uuid_indice.dict`

在对tfidf向量的矩阵存储方面, 我们选择使用稀疏矩阵的方法来降低对空间的占用. 因为对数据的分析后发现提取的文章中的词汇在的其他文章中出现的概率很小, 即对应的矩阵值为0. 矩阵的行对应的是提取的`word`单词, 列对应的是文章的id, 记录的内容是的文章中这个单词的`tfidf`值.

## 数据处理 build_indices.py
在数据处理部分使用`nltk`, `gensim`等python库, 来进行文章语言的处理. 

 `word_list = list(gensim.utils.tokenize(text, lowercase=True, deacc=True))   # tokenize`来进行分词操作

 当单词在停用词表时将其删除

 使用`porter_stemmer = nltk.stem.PorterStemmer()`, `porter_stemmer.stem(word)`对单词进行标准化处理.

在数据处理的同时利用得到的中间结果直接对每个文章的词汇进行`tfidf`处理. 即通过求得每篇文章处理后的词汇的个数.


## Bool检索 bool_search.py

*[酮负责语义转化] 输入限制格式 样例 结果

得到对输入`Bool`数据的语法树后, 进行Bool检索操作. 对语法树进行`def tree_to_stack(root: Tree)`将树通过**后序遍历**转化成栈存储起来.

在`def bool_search(indicesfile:str)`中对栈中的数据进行分析. 使用python中的`set`的数据结构对结果进行处理, `And`操作是对两个set集合进行`&`操作, `OR`操作对两个set集合进行`|`操作, NOT操作是对目标set和全集进行`fullset.difference(setstack[-1])`操作. 时间复杂度在实验数据中为`O(n+m), 两个set的元素个数`

最后的结果得到唯一的set集合, 即为得到的文章id的集合. 

## Tf-idf Semantic_Search.py
数据记录格式稀疏矩阵见上

在得到输入的查询词后, 经过标准化操作在矩阵中找到对应单词和文章的tfidf向量`d`, 在对查询词建立`tfidf向量``q`后, 进行`cosine`的相似度计算. 计算公式如下:

因此, 在计算过程中, 对于在稀疏矩阵上使用的优化方式是对查询词所对应的tfidf进行累加后得到结果. 引入了`TextInfo`数据类型, 对每个文章`text`都有对应的记录. 第一项是`self.dot`是累加查询词tfidf和文章对应词的tfidf的结果. 因为在文章中没有出现的查询词的tfidf都记录为0, 最后得到的结果是` `. 同理, 第二项记录的是文章对应词的tfidf的乘积和, 最后的结果是` `. 最后根据公式` ` 可以得到每个文章tfidf向量对于查询词tfidf向量的`cosine`, 选择最大的`cos`值对应的文章id和uuid即可得到结果. 

测试结果分析: 对实验给出的`searchwords.txt`的内容进行检测后得到的结果是` `. 并且根据文本分析, 文章中出现的查询词数量有70个, 并且出现次数为300次, 足以说明文章内容和查询词极为相关. 如图所示:



## 实验总结与分析
TODO!

## 实验进行的优化
我们使用`import multiprocessing` 使用多进程对多个文件进行处理(酮补充)   TODO!
结果图:

倒排索引的空间复杂度的优化, id -> uuid



